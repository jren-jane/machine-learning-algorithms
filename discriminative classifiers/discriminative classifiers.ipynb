{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1625241037918,
     "user": {
      "displayName": "Roger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVsR4mMeGc98ZcKHvtfMByG45D5AXKbvo6e2vMmW8=s64",
      "userId": "17884231074015833855"
     },
     "user_tz": -120
    },
    "id": "2CohoKCA6bnN"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import multi_dot\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import det\n",
    "import pandas as pd\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxilary classes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_size]\n",
    "    train_indices = shuffled_indices[test_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(df, mu=None, sigma=None):\n",
    "    # copy the dataframe\n",
    "    if df.size == 0:\n",
    "        return df, None, None\n",
    "    df_normalized = df.copy()\n",
    "    if mu is None and sigma is None:\n",
    "        mu = df_normalized.mean(axis=0)\n",
    "        sigma = df_normalized.std(axis=0, ddof=1)\n",
    "    df_normalized = (df_normalized - mu) / sigma\n",
    "    return df_normalized, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_minmax(df, min_value=None, minmax_gap=None):\n",
    "    # copy the dataframe\n",
    "    if df.size == 0:\n",
    "        return df, None, None\n",
    "    df_normalized = df.copy()\n",
    "    if min_value is None and minmax_gap is None:\n",
    "        min_value = df_normalized.min(axis=0)\n",
    "        minmax_gap = df_normalized.max(axis=0) - df_normalized.min(axis=0)\n",
    "    df_normalized = (df_normalized - min_value) / minmax_gap\n",
    "    return df_normalized, min_value, minmax_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "ZKhKgjU16bnR"
   },
   "outputs": [],
   "source": [
    "# This method transforms a group set vector into a binary group set matrix\n",
    "def transform_labels(classLabels):\n",
    "    new_label = np.zeros((len(classLabels), len(np.unique(classLabels))))\n",
    "    for i in range(len(classLabels)):\n",
    "        if classLabels[i] == 1:\n",
    "            new_label[i][0] = 1\n",
    "        elif classLabels[i] == 2:\n",
    "            new_label[i][1] = 1\n",
    "        else:\n",
    "            new_label[i][2] = 1\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_perceptron_sign(array):\n",
    "    for i in range(len(array)):\n",
    "        if array[i] == 0:\n",
    "            array[i] = -1\n",
    "    return array\n",
    "\n",
    "def loss(y, y_pred, type_of_loss=None, features_set=None, beta=None):\n",
    "    y = np.array(y).ravel()\n",
    "    y_pred = np.array(y_pred).ravel()\n",
    "    if type_of_loss == 'mse':\n",
    "        loss = (y - y_pred) ** 2 / len(y)\n",
    "        return loss\n",
    "    elif type_of_loss == '0-1':\n",
    "        # binary classification\n",
    "        # assume that wine type 1 is for positive, type 2 is for negative\n",
    "        # however, we have to reassign the group numbers if we classify wine type 3\n",
    "        if len(np.unique(y_pred)) <= 2:\n",
    "            loss = np.sum(y != y_pred)\n",
    "            # false positive: true 2 classified as 1\n",
    "            type_i_error = np.sum(y_pred < y)\n",
    "            # false negative: true 1 classified as 2\n",
    "            type_ii_error = np.sum(y < y_pred)\n",
    "            return loss, type_i_error, type_ii_error\n",
    "        else:\n",
    "            loss = np.sum(y != y_pred)\n",
    "            return loss\n",
    "    elif type_of_loss == 'hinge':\n",
    "        loss = 0\n",
    "        y = transform_perceptron_sign(y)\n",
    "        y_pred = transform_perceptron_sign(y_pred)\n",
    "        for i in range(len(y)):\n",
    "            if y[i] * y_pred[i] < 1:\n",
    "                loss += 1 - y[i] * np.dot(features_set[i].T, beta[0])\n",
    "        return loss\n",
    "    else: \n",
    "        message = 'type of error not specified'\n",
    "        return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier():\n",
    "    def __init__(self):\n",
    "        self.logit_param = None\n",
    "        self.class_labels = None\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "\n",
    "    def fit(self, features_set, group_set, class_labels, alpha=0.1, num_iter=250, reg_opt = None, lambda_ = 0.01):\n",
    "        complete_set = pd.concat([group_set, features_set], axis=1)\n",
    "        sorted_complete_set = pd.merge(class_labels, complete_set, on='Label', sort=False)\n",
    "        features_set = np.array(sorted_complete_set.iloc[:, 1:])\n",
    "        features_set, self.mu, self.sigma = feature_normalize(features_set)\n",
    "        group_set = np.array(sorted_complete_set.iloc[:, 0])\n",
    "        class_labels = np.array(class_labels).ravel()\n",
    "        sample_size = features_set.shape[0]\n",
    "        self.class_labels = class_labels\n",
    "        \n",
    "        if len(self.class_labels) == 2:\n",
    "            weight = np.zeros(features_set.shape[1])\n",
    "            bias = 0\n",
    "            class_labels_transform = (group_set==class_labels[0])\n",
    "            for i in range(num_iter):\n",
    "                sigma = 1 / (1 + np.exp(-np.dot(features_set, weight) - bias))\n",
    "                if reg_opt == 'l1': # Lasso regression\n",
    "                    gradient = np.dot(features_set.T, (sigma * np.ones(sample_size) - class_labels_transform)) / sample_size + lambda_ * np.sign(weight) \n",
    "                elif reg_opt == 'l2': # Ridge regression\n",
    "                    gradient = np.dot(features_set.T, (sigma * np.ones(sample_size) - class_labels_transform)) / sample_size + lambda_ * weight \n",
    "                else:\n",
    "                    gradient = np.dot(features_set.T, (sigma * np.ones(sample_size) - class_labels_transform)) / sample_size\n",
    "                #db = np.sum(sigma - group_set) / features_set.shape[0]\n",
    "                db = np.sum(sigma * np.ones(features_set.shape[0]) - (group_set==class_labels[0])) / sample_size\n",
    "                weight -= alpha * gradient\n",
    "                bias -= alpha * db\n",
    "        else:\n",
    "            weight = np.zeros((features_set.shape[1], len(np.unique(group_set))))\n",
    "            bias = 0\n",
    "            class_labels_transform = transform_labels(group_set)\n",
    "            for i in range(num_iter):\n",
    "                # sigma = np.exp(np.dot(featuresSet, weight) + bias) / 1 + np.sum(np.exp(np.dot(featuresSet, weight) + bias))\n",
    "                # sigma = softmax(np.dot(features_set, weight) + bias)\n",
    "                sigma = np.exp(np.dot(features_set, weight) + bias) / np.sum(np.exp(np.dot(features_set, weight) + bias), axis = 1).reshape(-1, 1)\n",
    "                if reg_opt == 'l1': # Lasso regression\n",
    "                    gradient = np.dot(features_set.T, (sigma - class_labels_transform)) / sample_size + lambda_ * np.sign(weight) \n",
    "                elif reg_opt == 'l2': # Ridge regression\n",
    "                    gradient = np.dot(features_set.T, (sigma - class_labels_transform)) / sample_size + lambda_ * weight\n",
    "                else:\n",
    "                    gradient = np.dot(features_set.T, (sigma - class_labels_transform)) / sample_size\n",
    "                db = np.sum(sigma - class_labels_transform) / features_set.shape[0]\n",
    "                weight -= alpha * gradient\n",
    "                bias -= alpha * db\n",
    "        self.logit_param = weight, bias\n",
    "        return self\n",
    "\n",
    "    def predict(self, features_set):\n",
    "        features_set = feature_normalize(features_set, self.mu, self.sigma)[0]\n",
    "        if len(self.class_labels) == 2:\n",
    "            result = 1 / (1 + np.exp(-np.dot(features_set, self.logit_param[0]) - self.logit_param[1]))\n",
    "            # y_pred = np.where(result >= 0.5, 1, 0)\n",
    "            y_pred = np.zeros(result.shape[0])\n",
    "            for i in range(len(y_pred)):\n",
    "                if result[i] >= 0.5:  # threshold = 0.5\n",
    "                    y_pred[i] = self.class_labels[0]\n",
    "                else:\n",
    "                    y_pred[i] = self.class_labels[1]\n",
    "        else:\n",
    "            # result = np.exp(np.dot(featuresSet, logit_param[0]) + logit_param[1]) / 1 + np.sum(np.exp(np.dot(featuresSet, logit_param[0]) + logit_param[1]))\n",
    "            # result = softmax(np.dot(features_set, self.logit_param[0]) + self.logit_param[1])\n",
    "            result = np.exp(np.dot(features_set, self.logit_param[0]) + self.logit_param[1]) / np.sum(np.exp(np.dot(features_set, self.logit_param[0]) + self.logit_param[1]), axis = 1).reshape(-1, 1)\n",
    "            y_pred = self.class_labels[np.argmax(result, axis=1)]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "KMi5Dk3Z6bnU"
   },
   "outputs": [],
   "source": [
    "def step(features_set, weight, bias):\n",
    "        z = np.dot(weight, features_set) + bias\n",
    "        if z >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "class Perceptron():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.class_labels = None\n",
    "        self.beta = None\n",
    "        self.min_value = None\n",
    "        self.minmax_gap = None\n",
    "\n",
    "    def fit(self, features_set, group_set, class_labels, alpha=0.1, num_iter=250):\n",
    "        complete_set = pd.concat([group_set, features_set], axis=1)\n",
    "        sorted_complete_set = pd.merge(class_labels, complete_set, on='Label', sort=False)\n",
    "        features_set = np.array(sorted_complete_set.iloc[:, 1:])\n",
    "        features_set, self.min_value, self.minmax_gap = feature_minmax(features_set)\n",
    "        group_set = np.array(sorted_complete_set.iloc[:, 0])\n",
    "        class_labels = np.array(class_labels).ravel()\n",
    "        self.class_labels = class_labels\n",
    "        class_labels_transform = (group_set==class_labels[0])\n",
    "        weight = np.zeros(features_set.shape[1])\n",
    "        bias = 0\n",
    "        for i in range(num_iter):\n",
    "            for m in range(len(features_set)):\n",
    "                y_ = step(features_set[m], weight, bias)\n",
    "                for n in range(len(weight)):\n",
    "                    weight[n] += alpha * (class_labels_transform[m] - y_) * features_set[m][n]\n",
    "                    bias += alpha * (class_labels_transform[m] - y_)\n",
    "        self.beta = weight, bias\n",
    "        return self\n",
    "\n",
    "    def predict(self, features_set):\n",
    "        features_set = feature_normalize(features_set, self.min_value, self.minmax_gap)[0]\n",
    "        result = np.dot(features_set, self.beta[0]) + self.beta[1]\n",
    "        y_pred = np.zeros(result.shape[0])\n",
    "        for i in range(len(y_pred)):\n",
    "            if result[i] >= 0:\n",
    "                y_pred[i] = self.class_labels[0]\n",
    "            else:\n",
    "                y_pred[i] = self.class_labels[1]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_fit(train_sample, train_class, test, k):\n",
    "    train_sample, mu, sigma = feature_normalize(train_sample)\n",
    "    train_sample = np.array(train_sample)\n",
    "    train_class = np.array(train_class).ravel().astype(np.int)\n",
    "    test = feature_normalize(test, mu, sigma)[0]\n",
    "    test = np.array(test)\n",
    "    sample_size = train_sample.shape[0]\n",
    "    test_size = test.shape[0]\n",
    "    result = []\n",
    "    for i in range(test_size):\n",
    "        distance_all = []\n",
    "        for j in range(sample_size):\n",
    "            distance = np.sum((test[i] - train_sample[j])**2)\n",
    "            distance_all.append(distance)\n",
    "        neighbours_index = np.argsort(distance_all)[0 : k]\n",
    "        class_counts = np.bincount(train_class[neighbours_index])\n",
    "        most_frequent_class = np.argmax(class_counts)\n",
    "        result.append(most_frequent_class)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXpgZ7ge6bnX"
   },
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('wine.csv', header = None)\n",
    "column_names = ['Label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash',\n",
    "                'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols',\n",
    "                'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "                'Proline']\n",
    "wine = pd.DataFrame(data)\n",
    "wine.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample, test_sample = train_test_split(wine, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose predictors\n",
    "train_features_set = train_sample[train_sample['Label'].isin([1, 3])][['Alcalinity of ash','Proanthocyanins']]\n",
    "train_group_set = train_sample[train_sample['Label'].isin([1, 3])][['Label']]\n",
    "class_labels = pd.DataFrame([1, 3], columns=['Label'])\n",
    "test_features_set = test_sample[test_sample['Label'].isin([1, 3])][['Alcalinity of ash','Proanthocyanins']]\n",
    "test_group_set = test_sample[test_sample['Label'].isin([1, 3])].iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing regularization parameter using cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n"
     ]
    }
   ],
   "source": [
    "n_train = train_features_set.shape[0]\n",
    "num_fold = 5\n",
    "n_fold = math.floor(n_train / num_fold)\n",
    "\n",
    "total_error_all = []\n",
    "type_i_error_all = []\n",
    "type_ii_error_all = []\n",
    "meshgrid = np.arange(0, 0.1, 0.01)\n",
    "\n",
    "features_set = train_features_set\n",
    "group_set = train_group_set\n",
    "\n",
    "for lambda_ in meshgrid:\n",
    "    total_error = 0\n",
    "    type_i_error = 0\n",
    "    type_ii_error = 0\n",
    "    for cur_fold in range(num_fold):\n",
    "        features_set_fold = features_set.iloc[cur_fold * n_fold : (cur_fold + 1) * n_fold]\n",
    "        group_set_fold = group_set.iloc[cur_fold * n_fold : (cur_fold + 1) * n_fold]\n",
    "        features_set_train_fold = features_set.iloc[np.r_[0 : cur_fold * n_fold, (cur_fold + 1) * n_fold : n_train]]\n",
    "        group_set_train_fold = group_set.iloc[np.r_[0 : cur_fold * n_fold, (cur_fold + 1) * n_fold : n_train]]\n",
    "\n",
    "        pr = LogisticClassifier().fit(features_set_train_fold, group_set_train_fold, class_labels, reg_opt='l1', lambda_ = lambda_)\n",
    "        group_set_pred = pr.predict(features_set_fold)\n",
    "        total_error += loss(group_set_fold, group_set_pred, '0-1')[0]\n",
    "        type_i_error += loss(group_set_fold, group_set_pred, '0-1')[1]\n",
    "        type_ii_error += loss(group_set_fold, group_set_pred, '0-1')[2]\n",
    "    total_error_all.append(total_error)\n",
    "    type_i_error_all.append(type_i_error)\n",
    "    type_ii_error_all.append(type_ii_error)\n",
    "    \n",
    "best_lambda = meshgrid[np.argsort(total_error_all)[0]]\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n"
     ]
    }
   ],
   "source": [
    "n_train = train_features_set.shape[0]\n",
    "num_fold = 5\n",
    "n_fold = math.floor(n_train / num_fold)\n",
    "\n",
    "total_error_all = []\n",
    "type_i_error_all = []\n",
    "type_ii_error_all = []\n",
    "meshgrid = np.arange(0, 0.1, 0.01)\n",
    "\n",
    "features_set = train_features_set\n",
    "group_set = train_group_set\n",
    "\n",
    "for lambda_ in meshgrid:\n",
    "    total_error = 0\n",
    "    type_i_error = 0\n",
    "    type_ii_error = 0\n",
    "    for cur_fold in range(num_fold):\n",
    "        features_set_fold = features_set.iloc[cur_fold * n_fold : (cur_fold + 1) * n_fold]\n",
    "        group_set_fold = group_set.iloc[cur_fold * n_fold : (cur_fold + 1) * n_fold]\n",
    "        features_set_train_fold = features_set.iloc[np.r_[0 : cur_fold * n_fold, (cur_fold + 1) * n_fold : n_train]]\n",
    "        group_set_train_fold = group_set.iloc[np.r_[0 : cur_fold * n_fold, (cur_fold + 1) * n_fold : n_train]]\n",
    "\n",
    "        pr = LogisticClassifier().fit(features_set_train_fold, group_set_train_fold, class_labels, reg_opt='l2', lambda_ = lambda_)\n",
    "        group_set_pred = pr.predict(features_set_fold)\n",
    "        total_error += loss(group_set_fold, group_set_pred, '0-1')[0]\n",
    "        type_i_error += loss(group_set_fold, group_set_pred, '0-1')[1]\n",
    "        type_ii_error += loss(group_set_fold, group_set_pred, '0-1')[2]\n",
    "    total_error_all.append(total_error)\n",
    "    type_i_error_all.append(type_i_error)\n",
    "    type_ii_error_all.append(type_ii_error)\n",
    "    \n",
    "best_lambda = meshgrid[np.argsort(total_error_all)[0]]\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing error without the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticClassifier().fit(train_features_set, train_group_set, class_labels)\n",
    "train_group_set_pred = lr.predict(train_features_set)\n",
    "test_group_set_pred = lr.predict(test_features_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error      7\n",
      "type_i_error     4\n",
      "type_ii_error    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(train_group_set, train_group_set_pred, '0-1'), index = ['total_error', 'type_i_error', 'type_ii_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error      2\n",
      "type_i_error     1\n",
      "type_ii_error    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(test_group_set, test_group_set_pred, '0-1'), index = ['total_error', 'type_i_error', 'type_ii_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing error for Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticClassifier().fit(train_features_set, train_group_set, class_labels, reg_opt = 'l1', lambda_ = 0.02)\n",
    "train_group_set_pred = lr.predict(train_features_set)\n",
    "test_group_set_pred = lr.predict(test_features_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error      7\n",
      "type_i_error     4\n",
      "type_ii_error    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(train_group_set, train_group_set_pred, '0-1'), index = ['total_error', 'type_i_error', 'type_ii_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error      2\n",
      "type_i_error     1\n",
      "type_ii_error    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(test_group_set, test_group_set_pred, '0-1'), index = ['total_error', 'type_i_error', 'type_ii_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing error for Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticClassifier().fit(train_features_set, train_group_set, class_labels, reg_opt = 'l2', lambda_ = 0.02)\n",
    "train_group_set_pred = lr.predict(train_features_set)\n",
    "test_group_set_pred = lr.predict(test_features_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error      7\n",
      "type_i_error     4\n",
      "type_ii_error    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(train_group_set, train_group_set_pred, '0-1'), index = ['total_error', 'type_i_error', 'type_ii_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error      2\n",
      "type_i_error     1\n",
      "type_ii_error    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(test_group_set, test_group_set_pred, '0-1'), index = ['total_error', 'type_i_error', 'type_ii_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification result plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "lr_plot_1 = train_features_set.iloc[:, 0]\n",
    "lr_plot_2 = train_group_set_pred\n",
    "lr_plot = np.column_stack((lr_plot_1, lr_plot_2))\n",
    "lr_plot_x1 = lr_plot[lr_plot[:, 1] == 1]\n",
    "lr_plot_x2 = lr_plot[lr_plot[:, 1] == 3]\n",
    "\n",
    "zeros1 = np.zeros((len(lr_plot_x1[:, 0]), 1))\n",
    "zeros2 = np.zeros((len(lr_plot_x2[:, 0]), 1))\n",
    "plt.hist(lr_plot_x1[:, 0], color='red')\n",
    "plt.hist(lr_plot_x2[:, 0], color='blue')\n",
    "\n",
    "plt.title('lr classification - feature 1')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron().fit(train_features_set, train_group_set, class_labels)\n",
    "train_group_set_pred = p.predict(train_features_set)\n",
    "test_group_set_pred = p.predict(test_features_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error      0\n",
      "type_i_error     0\n",
      "type_ii_error    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(train_group_set, train_group_set_pred, 'hinge', train_features_set, p.beta), index = ['total_error', 'type_i_error', 'type_ii_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error      0\n",
      "type_i_error     0\n",
      "type_ii_error    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(test_group_set, test_group_set_pred, 'hinge', test_features_set, p.beta), index = ['total_error', 'type_i_error', 'type_ii_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of neighbours using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "n_train = train_features_set.shape[0]\n",
    "num_fold = 10\n",
    "n_fold = math.floor(n_train / num_fold)\n",
    "\n",
    "total_error_all = []\n",
    "type_i_error_all = []\n",
    "type_ii_error_all = []\n",
    "meshgrid = np.arange(1, 30, 1)\n",
    "\n",
    "features_set = train_features_set\n",
    "group_set = train_group_set\n",
    "\n",
    "for k in meshgrid:\n",
    "    total_error = 0\n",
    "    type_i_error = 0\n",
    "    type_ii_error = 0\n",
    "    for cur_fold in range(num_fold):\n",
    "        features_set_fold = features_set.iloc[cur_fold * n_fold : (cur_fold + 1) * n_fold]\n",
    "        group_set_fold = group_set.iloc[cur_fold * n_fold : (cur_fold + 1) * n_fold]\n",
    "        features_set_train_fold = features_set.iloc[np.r_[0 : cur_fold * n_fold, (cur_fold + 1) * n_fold : n_train]]\n",
    "        group_set_train_fold = group_set.iloc[np.r_[0 : cur_fold * n_fold, (cur_fold + 1) * n_fold : n_train]]\n",
    "\n",
    "        group_set_pred = KNN_fit(features_set_train_fold, group_set_train_fold, features_set_fold, k)\n",
    "        total_error += loss(group_set_fold, group_set_pred, '0-1')[0]\n",
    "        type_i_error += loss(group_set_fold, group_set_pred, '0-1')[1]\n",
    "        type_ii_error += loss(group_set_fold, group_set_pred, '0-1')[2]\n",
    "    total_error_all.append(total_error)\n",
    "    type_i_error_all.append(type_i_error)\n",
    "    type_ii_error_all.append(type_ii_error)\n",
    "    \n",
    "best_k = meshgrid[np.argsort(total_error_all)[0]]\n",
    "print(best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_group_pred = KNN_fit(train_features_set, train_group_set, train_features_set, 29)\n",
    "test_group_pred = KNN_fit(train_features_set, train_group_set, test_features_set, 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error      6\n",
      "type_i_error     4\n",
      "type_ii_error    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(train_group_set, train_group_pred, '0-1'), index = ['total_error', 'type_i_error', 'type_ii_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error      2\n",
      "type_i_error     1\n",
      "type_ii_error    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(test_group_set, test_group_pred, '0-1'), index = ['total_error', 'type_i_error', 'type_ii_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Classes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose predictors\n",
    "train_features_set = train_sample[['Alcohol', 'Flavanoids', 'Proanthocyanins', 'Color intensity']]\n",
    "train_group_set = train_sample[['Label']]\n",
    "class_labels = pd.DataFrame([1, 2, 3], columns=['Label'])\n",
    "test_features_set = test_sample[['Alcohol', 'Flavanoids', 'Proanthocyanins', 'Color intensity']]\n",
    "test_group_set = test_sample[['Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing regularization parameter using cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "n_train = train_features_set.shape[0]\n",
    "num_fold = 5\n",
    "n_fold = math.floor(n_train / num_fold)\n",
    "\n",
    "total_error_all = []\n",
    "type_i_error_all = []\n",
    "type_ii_error_all = []\n",
    "meshgrid = np.arange(0, 0.1, 0.01)\n",
    "\n",
    "features_set = train_features_set\n",
    "group_set = train_group_set\n",
    "\n",
    "for lambda_ in meshgrid:\n",
    "    total_error = 0\n",
    "    for cur_fold in range(num_fold):\n",
    "        features_set_fold = features_set.iloc[cur_fold * n_fold : (cur_fold + 1) * n_fold]\n",
    "        group_set_fold = group_set.iloc[cur_fold * n_fold : (cur_fold + 1) * n_fold]\n",
    "        features_set_train_fold = features_set.iloc[np.r_[0 : cur_fold * n_fold, (cur_fold + 1) * n_fold : n_train]]\n",
    "        group_set_train_fold = group_set.iloc[np.r_[0 : cur_fold * n_fold, (cur_fold + 1) * n_fold : n_train]]\n",
    "\n",
    "        pr = LogisticClassifier().fit(features_set_train_fold, group_set_train_fold, class_labels, reg_opt='l1', lambda_ = lambda_)\n",
    "        group_set_pred = pr.predict(features_set_fold)\n",
    "        total_error += loss(group_set_fold, group_set_pred, '0-1')\n",
    "    total_error_all.append(total_error)\n",
    "    \n",
    "best_lambda = meshgrid[np.argsort(total_error_all)[0]]\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "n_train = train_features_set.shape[0]\n",
    "num_fold = 5\n",
    "n_fold = math.floor(n_train / num_fold)\n",
    "\n",
    "total_error_all = []\n",
    "type_i_error_all = []\n",
    "type_ii_error_all = []\n",
    "meshgrid = np.arange(0, 0.1, 0.01)\n",
    "\n",
    "features_set = train_features_set\n",
    "group_set = train_group_set\n",
    "\n",
    "for lambda_ in meshgrid:\n",
    "    total_error = 0\n",
    "    for cur_fold in range(num_fold):\n",
    "        features_set_fold = features_set.iloc[cur_fold * n_fold : (cur_fold + 1) * n_fold]\n",
    "        group_set_fold = group_set.iloc[cur_fold * n_fold : (cur_fold + 1) * n_fold]\n",
    "        features_set_train_fold = features_set.iloc[np.r_[0 : cur_fold * n_fold, (cur_fold + 1) * n_fold : n_train]]\n",
    "        group_set_train_fold = group_set.iloc[np.r_[0 : cur_fold * n_fold, (cur_fold + 1) * n_fold : n_train]]\n",
    "\n",
    "        pr = LogisticClassifier().fit(features_set_train_fold, group_set_train_fold, class_labels, reg_opt='l2', lambda_ = lambda_)\n",
    "        group_set_pred = pr.predict(features_set_fold)\n",
    "        total_error += loss(group_set_fold, group_set_pred, '0-1')\n",
    "    total_error_all.append(total_error)\n",
    "    \n",
    "best_lambda = meshgrid[np.argsort(total_error_all)[0]]\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing error without regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticClassifier().fit(train_features_set, train_group_set, class_labels)\n",
    "train_group_set_pred = lr.predict(train_features_set)\n",
    "test_group_set_pred = lr.predict(test_features_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error    10\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(train_group_set, train_group_set_pred, '0-1'), index = ['total_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error    2\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "result = pd.Series(loss(test_group_set, test_group_set_pred, '0-1'), index = ['total_error'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML homework04.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "04b0022e25506edf1123745eb6a5641eeb58f4c6825d53cb9267480276f19e42"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
